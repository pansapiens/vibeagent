# Copy this file to .env and fill in the values

# OpenRouter
OPENAI_API_BASE=https://openrouter.ai/api/v1
OPENAI_API_KEY=sk-or-v1-your-openrouter-key-here
MODEL=mistralai/mistral-small-3.2-24b-instruct-2506:free
#MODEL=mistralai/devstral-small:free
#MODEL=google/gemma-3n-e4b-it:free

# A local OpenAI-compatible server like llama.cpp, llama-swap, etc.
#OPENAI_API_KEY=sk-your_key_here
# (my llama-swap port is 9292)
#OPENAI_API_BASE=http://127.0.0.1:9292/v1
#MODEL=Jan-nano-128k-UD:Q8_K_XL

# A local ollama instance
#OPENAI_API_KEY=your_ollama_key_might_not_matter
#OPENAI_API_BASE=http://127.0.0.1:11434
#MODEL=qwen3:8b

# Telemetry (Phoenix/OpenTelemetry)
#OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
#DISABLE_TELEMETRY=false